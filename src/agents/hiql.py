import copy

from jaxrl_m.typing import *

import jax
import jax.numpy as jnp
import numpy as np
import optax
from jaxrl_m.common import TrainState, target_update
from jaxrl_m.networks import Policy, Critic, ensemblize, DiscretePolicy

import flax
import flax.linen as nn
from flax.core import freeze, unfreeze
import ml_collections
from . import iql
from src.special_networks import (
    Representation,
    HierarchicalActorCritic,
    RelativeRepresentation,
    MonolithicVF,
    FrozenTDMPCEncoder,
    OneLayer,
)


def expectile_loss(adv, diff, expectile=0.7):
    weight = jnp.where(adv >= 0, expectile, (1 - expectile))
    return weight * (diff**2)


def compute_actor_loss(agent, batch, network_params):
    if agent.config[
        "use_waypoints"
    ]:  # Use waypoint states as goals (for hierarchical policies)
        cur_goals = batch["low_goals"]
    else:  # Use randomized last observations as goals (for flat policies)
        cur_goals = batch["high_goals"]
    v1, v2 = agent.network(batch["observations"], cur_goals, method="value")
    nv1, nv2 = agent.network(batch["next_observations"], cur_goals, method="value")
    v = (v1 + v2) / 2
    nv = (nv1 + nv2) / 2

    adv = nv - v
    exp_a = jnp.exp(adv * agent.config["temperature"])
    exp_a = jnp.minimum(exp_a, 100.0)

    if agent.config["use_waypoints"]:
        goal_rep_grad = agent.config["policy_train_rep"]
    else:
        goal_rep_grad = True
    dist = agent.network(
        batch["observations"],
        cur_goals,
        state_rep_grad=True,
        goal_rep_grad=goal_rep_grad,
        method="actor",
        params=network_params,
    )

    log_probs = dist.log_prob(batch["actions"])
    actor_loss = -(exp_a * log_probs).mean()

    # # Use jax.lax.cond to conditionally handle NaN values
    # def check_nan(x):
    #     print("Found NaN in actor loss")
    #     return x

    # def no_nan(x):
    #     return x

    # actor_loss = jax.lax.cond(
    #     jnp.any(jnp.isnan(actor_loss)),
    #     check_nan,
    #     no_nan,
    #     actor_loss,
    # )

    return actor_loss, {
        "actor_loss": actor_loss,
        "adv": adv.mean(),
        "bc_log_probs": log_probs.mean(),
        "adv_median": jnp.median(adv),
        "mse": jnp.mean((dist.mode() - batch["actions"]) ** 2),
    }


def compute_high_actor_loss(agent, batch, network_params):
    cur_goals = batch["high_goals"]
    v1, v2 = agent.network(batch["observations"], cur_goals, method="value")
    nv1, nv2 = agent.network(batch["high_targets"], cur_goals, method="value")
    v = (v1 + v2) / 2
    nv = (nv1 + nv2) / 2

    adv = nv - v
    exp_a = jnp.exp(adv * agent.config["high_temperature"])
    exp_a = jnp.minimum(exp_a, 100.0)

    dist = agent.network(
        batch["observations"],
        batch["high_goals"],
        state_rep_grad=True,
        goal_rep_grad=True,
        method="high_actor",
        params=network_params,
    )
    if agent.config["use_rep"]:
        target = agent.network(
            targets=batch["high_targets"],
            bases=batch["observations"],
            method="value_goal_encoder",
        )
    else:
        target = batch["high_targets"] - batch["observations"]
    log_probs = dist.log_prob(target)
    actor_loss = -(exp_a * log_probs).mean()

    return actor_loss, {
        "high_actor_loss": actor_loss,
        "high_adv": adv.mean(),
        "high_bc_log_probs": log_probs.mean(),
        "high_adv_median": jnp.median(adv),
        "high_mse": jnp.mean((dist.mode() - target) ** 2),
        "high_scale": dist.scale_diag.mean(),
    }


def compute_value_loss(agent, batch, network_params):
    # masks are 0 if terminal, 1 otherwise
    batch["masks"] = 1.0 - batch["rewards"]
    # rewards are 0 if terminal, -1 otherwise
    batch["rewards"] = batch["rewards"] - 1.0

    (next_v1, next_v2) = agent.network(
        batch["next_observations"], batch["goals"], method="target_value"
    )
    next_v = jnp.minimum(next_v1, next_v2)
    q = batch["rewards"] + agent.config["discount"] * batch["masks"] * next_v

    (v1_t, v2_t) = agent.network(
        batch["observations"], batch["goals"], method="target_value"
    )
    v_t = (v1_t + v2_t) / 2
    adv = q - v_t

    q1 = batch["rewards"] + agent.config["discount"] * batch["masks"] * next_v1
    q2 = batch["rewards"] + agent.config["discount"] * batch["masks"] * next_v2
    (v1, v2) = agent.network(
        batch["observations"], batch["goals"], method="value", params=network_params
    )

    value_loss1 = expectile_loss(
        adv, q1 - v1, agent.config["pretrain_expectile"]
    ).mean()
    value_loss2 = expectile_loss(
        adv, q2 - v2, agent.config["pretrain_expectile"]
    ).mean()
    value_loss = value_loss1 + value_loss2

    advantage = adv
    return value_loss, {
        "value_loss": value_loss,
        "v max": v1.max(),
        "v min": v1.min(),
        "v mean": v1.mean(),
        "abs adv mean": jnp.abs(advantage).mean(),
        "adv mean": advantage.mean(),
        "adv max": advantage.max(),
        "adv min": advantage.min(),
        "accept prob": (advantage >= 0).mean(),
    }


class JointTrainAgent(iql.IQLAgent):
    network: TrainState = None

    def pretrain_update(
        agent,
        pretrain_batch,
        seed=None,
        value_update=True,
        actor_update=True,
        high_actor_update=True,
    ):
        def loss_fn(network_params):
            info = {}

            # Value
            if value_update:
                value_loss, value_info = compute_value_loss(
                    agent, pretrain_batch, network_params
                )
                for k, v in value_info.items():
                    info[f"value/{k}"] = v
            else:
                value_loss = 0.0

            # Actor
            if actor_update:
                actor_loss, actor_info = compute_actor_loss(
                    agent, pretrain_batch, network_params
                )
                for k, v in actor_info.items():
                    info[f"actor/{k}"] = v
            else:
                actor_loss = 0.0

            # High Actor
            if high_actor_update and agent.config["use_waypoints"]:
                high_actor_loss, high_actor_info = compute_high_actor_loss(
                    agent, pretrain_batch, network_params
                )
                for k, v in high_actor_info.items():
                    info[f"high_actor/{k}"] = v
            else:
                high_actor_loss = 0.0

            loss = value_loss + actor_loss + high_actor_loss

            return loss, info

        if value_update:
            new_target_params = jax.tree_map(
                lambda p, tp: p * agent.config["target_update_rate"]
                + tp * (1 - agent.config["target_update_rate"]),
                agent.network.params["networks_value"],
                agent.network.params["networks_target_value"],
            )

        new_network, info = agent.network.apply_loss_fn(loss_fn=loss_fn, has_aux=True)

        if value_update:
            params = unfreeze(new_network.params)
            params["networks_target_value"] = new_target_params
            new_network = new_network.replace(params=freeze(params))

        return agent.replace(network=new_network), info

    pretrain_update = jax.jit(
        pretrain_update,
        static_argnames=("value_update", "actor_update", "high_actor_update"),
    )

    def sample_actions(
        agent,
        observations: np.ndarray,
        goals: np.ndarray,
        *,
        low_dim_goals: bool = False,
        seed: PRNGKey,
        temperature: float = 1.0,
        discrete: int = 0,
        num_samples: int = None,
    ) -> jnp.ndarray:
        dist = agent.network(
            observations,
            goals,
            low_dim_goals=low_dim_goals,
            temperature=temperature,
            method="actor",
        )
        if num_samples is None:
            actions = dist.sample(seed=seed)
        else:
            actions = dist.sample(seed=seed, sample_shape=num_samples)
        if not discrete:
            actions = jnp.clip(actions, -1, 1)
        return actions

    sample_actions = jax.jit(
        sample_actions, static_argnames=("num_samples", "low_dim_goals", "discrete")
    )

    def sample_high_actions(
        agent,
        observations: np.ndarray,
        goals: np.ndarray,
        *,
        seed: PRNGKey,
        temperature: float = 1.0,
        num_samples: int = None,
    ) -> jnp.ndarray:
        dist = agent.network(
            observations, goals, temperature=temperature, method="high_actor"
        )
        if num_samples is None:
            actions = dist.sample(seed=seed)
        else:
            actions = dist.sample(seed=seed, sample_shape=num_samples)
        return actions

    sample_high_actions = jax.jit(sample_high_actions, static_argnames=("num_samples",))

    @jax.jit
    def get_policy_rep(
        agent,
        *,
        targets: np.ndarray,
        bases: np.ndarray = None,
    ) -> jnp.ndarray:
        return agent.network(targets=targets, bases=bases, method="policy_goal_encoder")


def create_learner(
    seed: int,
    observations: jnp.ndarray,
    actions: jnp.ndarray,
    lr: float = 3e-4,
    actor_hidden_dims: Sequence[int] = (256, 256),
    value_hidden_dims: Sequence[int] = (256, 256),
    discount: float = 0.99,
    tau: float = 0.005,
    temperature: float = 1,
    high_temperature: float = 1,
    pretrain_expectile: float = 0.7,
    way_steps: int = 0,
    rep_dim: int = 10,
    use_rep: int = 0,
    policy_train_rep: float = 0,
    visual: int = 0,
    encoder: str = "impala",
    discrete: int = 0,
    use_layer_norm: int = 0,
    rep_type: str = "state",
    use_waypoints: int = 0,
    enc_path: str = None,
    **kwargs,
):

    print("Extra kwargs:", kwargs)

    rng = jax.random.PRNGKey(seed)
    rng, actor_key, high_actor_key, critic_key, value_key = jax.random.split(rng, 5)

    value_state_encoder = None
    value_goal_encoder = None
    policy_state_encoder = None
    policy_goal_encoder = None
    high_policy_state_encoder = None
    high_policy_goal_encoder = None
    if visual:
        assert use_rep
        from jaxrl_m.vision import encoders

        visual_encoder = encoders[encoder]
        obs_shape = observations.shape[-1]

        def make_encoder(bottleneck):
            if bottleneck:
                return RelativeRepresentation(
                    rep_dim=rep_dim,
                    hidden_dims=(rep_dim,),
                    visual=True,
                    module=visual_encoder,
                    layer_norm=use_layer_norm,
                    rep_type=rep_type,
                    bottleneck=True,
                )
            else:
                return RelativeRepresentation(
                    rep_dim=value_hidden_dims[-1],
                    hidden_dims=(value_hidden_dims[-1],),
                    visual=True,
                    module=visual_encoder,
                    layer_norm=use_layer_norm,
                    rep_type=rep_type,
                    bottleneck=False,
                )

        value_state_encoder = make_encoder(bottleneck=False)
        value_goal_encoder = make_encoder(bottleneck=use_waypoints)
        policy_state_encoder = make_encoder(bottleneck=False)
        policy_goal_encoder = make_encoder(bottleneck=False)
        high_policy_state_encoder = make_encoder(bottleneck=False)
        high_policy_goal_encoder = make_encoder(bottleneck=False)
    else:
        if enc_path is not None:
            rep_encoder = lambda: FrozenTDMPCEncoder(
                frozen_params=FrozenTDMPCEncoder.load_frozen_weights(enc_path),
            )
            obs_shape = 512
        else:
            rep_encoder = None
            obs_shape = observations.shape[-1]

        def make_encoder(bottleneck):
            if bottleneck:
                return RelativeRepresentation(
                    rep_dim=rep_dim,
                    hidden_dims=(*value_hidden_dims, rep_dim),
                    layer_norm=use_layer_norm,
                    rep_type=rep_type,
                    bottleneck=True,
                    rep_encoder=rep_encoder,
                )
            else:
                return RelativeRepresentation(
                    rep_dim=value_hidden_dims[-1],
                    hidden_dims=(*value_hidden_dims, value_hidden_dims[-1]),
                    layer_norm=use_layer_norm,
                    rep_type=rep_type,
                    bottleneck=False,
                    rep_encoder=rep_encoder,
                )

        if use_rep and rep_encoder is None:
            value_goal_encoder = make_encoder(bottleneck=True)
        elif rep_encoder is not None:
            value_goal_encoder = make_encoder(bottleneck=use_waypoints)
            value_state_encoder = make_encoder(bottleneck=False)
            policy_state_encoder = make_encoder(bottleneck=False)
            policy_goal_encoder = make_encoder(bottleneck=False)
            high_policy_state_encoder = make_encoder(bottleneck=False)
            high_policy_goal_encoder = make_encoder(bottleneck=False)

    value_def = MonolithicVF(
        hidden_dims=value_hidden_dims, use_layer_norm=use_layer_norm, rep_dim=rep_dim
    )

    if discrete:
        action_dim = actions[0] + 1
        actor_def = DiscretePolicy(actor_hidden_dims, action_dim=action_dim)
    else:
        action_dim = actions.shape[-1]
        actor_def = Policy(
            actor_hidden_dims,
            action_dim=action_dim,
            log_std_min=-5.0,
            state_dependent_std=False,
            tanh_squash_distribution=False,
        )

    high_action_dim = obs_shape if not use_rep else rep_dim
    high_actor_def = Policy(
        actor_hidden_dims,
        action_dim=high_action_dim,
        log_std_min=-5.0,
        state_dependent_std=False,
        tanh_squash_distribution=False,
    )

    network_def = HierarchicalActorCritic(
        encoders={
            "rep": None,
            "value_state": value_state_encoder,
            "value_goal": value_goal_encoder,
            "policy_state": policy_state_encoder,
            "policy_goal": policy_goal_encoder,
            "high_policy_state": high_policy_state_encoder,
            "high_policy_goal": high_policy_goal_encoder,
        },
        networks={
            "value": value_def,
            "target_value": copy.deepcopy(value_def),
            "actor": actor_def,
            "high_actor": high_actor_def,
        },
        use_waypoints=use_waypoints,
    )
    network_tx = optax.chain(optax.zero_nans(), optax.adam(learning_rate=lr))
    network_params = network_def.init(value_key, observations, observations)["params"]
    network = TrainState.create(network_def, network_params, tx=network_tx)
    params = unfreeze(network.params)
    params["networks_target_value"] = params["networks_value"]
    network = network.replace(params=freeze(params))

    config = flax.core.FrozenDict(
        dict(
            discount=discount,
            temperature=temperature,
            high_temperature=high_temperature,
            target_update_rate=tau,
            pretrain_expectile=pretrain_expectile,
            way_steps=way_steps,
            rep_dim=rep_dim,
            policy_train_rep=policy_train_rep,
            use_rep=use_rep,
            use_waypoints=use_waypoints,
        )
    )

    return JointTrainAgent(
        rng,
        network=network,
        critic=None,
        value=None,
        target_value=None,
        actor=None,
        config=config,
    )


def get_default_config():
    config = ml_collections.ConfigDict(
        {
            "lr": 3e-4,
            "actor_hidden_dims": (256, 256),
            "value_hidden_dims": (256, 256),
            "discount": 0.99,
            "temperature": 1.0,
            "tau": 0.005,
            "pretrain_expectile": 0.7,
        }
    )

    return config
